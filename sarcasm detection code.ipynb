{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "import regex as re\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import gzip\n",
    "\n",
    "# Modelling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Data Twitter LABELED.csv'\n",
    "df = pd.read_csv(file, sep=';', header=None, names=[\"Timestamp\", \"text\", \"sarcasm\"], encoding='iso-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove numbers\n",
    "    text = demoji.replace(text, '')  # Remove emoji\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "print(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['casefolded_text'] = df['cleaned_text'].str.lower()\n",
    "print(df['casefolded_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text'] = df['casefolded_text'].apply(nltk.word_tokenize)\n",
    "print(df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined_slang_words.txt', 'r') as file:\n",
    "    slang_dict = eval(file.read())\n",
    "\n",
    "def normalize_text(tokenized_text):\n",
    "    return [slang_dict[word] if word in slang_dict else word for word in tokenized_text]\n",
    "\n",
    "df['normalized_text'] = df['tokenized_text'].apply(normalize_text)\n",
    "print(df['normalized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(normalized_text):\n",
    "    stop_words = set(stopwords.words(\"indonesian\"))\n",
    "    return [word for word in normalized_text if word not in stop_words]\n",
    "\n",
    "df['stpwrdrmv_text'] = df['normalized_text'].apply(remove_stopwords)\n",
    "print(df[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stem_text(normalized_text):\n",
    "    return [stemmer.stem(word) for word in normalized_text]\n",
    "\n",
    "df['stemmed_text'] = df['stpwrdrmv_text'].apply(stem_text)\n",
    "print(df['stemmed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_text'] = df['stemmed_text'].apply(lambda x: ' '.join(x))\n",
    "print(df['final_text'])\n",
    "df.to_csv('final_text.csv', index=False)\n",
    "\n",
    "file = 'final_text.csv'\n",
    "df = pd.read_csv(file)\n",
    "df['final_text'] = df['final_text'].astype(str)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "def create_embedding_matrix(vocab_and_vectors, word_index, embedding_dim):\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        vector = vocab_and_vectors.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# FastText embedding dimensions\n",
    "embedding_dim = 300\n",
    "trunc_type = 'pre'\n",
    "padding_type = 'pre'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Initialize Tokenizer and Word Index\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "# Define the maximum sequence length\n",
    "tokenizer.fit_on_texts(df['final_text'])\n",
    "X_sequences = tokenizer.texts_to_sequences(df['final_text'])\n",
    "max_length = max(len(x) for x in X_sequences)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df['sarcasm'])\n",
    "\n",
    "# FastText model for Indonesian language\n",
    "file = 'cc.id.300.vec.gz'\n",
    "print('\\nWord Embedding in Process . . .')\n",
    "with gzip.open(file, 'rt', encoding='utf-8') as file:\n",
    "    vocab_and_vectors = {}  # Map words to vectors\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        vocab_and_vectors[word] = vector\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(vocab_and_vectors, tokenizer.word_index, embedding_dim)\n",
    "print('Word Embedding Done')\n",
    "\n",
    "# Display an example of word indexing\n",
    "print(\"Original Text:\\n\", df['final_text'][2])\n",
    "print(\"\\nIndexed Sequence:\\n\", X_sequences[2])\n",
    "\n",
    "# Display an example of padded sequence\n",
    "X_padded = pad_sequences([X_sequences[2]], maxlen=max_length)\n",
    "print(\"\\nPadded Sequence:\\n\", X_padded)\n",
    "\n",
    "# Display an example of word embedding\n",
    "word_index = tokenizer.word_index\n",
    "example_word = list(word_index.keys())[1]\n",
    "example_embedding = vocab_and_vectors.get(example_word)\n",
    "print(\"\\nExample Word Embedding for '{}':\\n\".format(example_word), example_embedding)\n",
    "\n",
    "# Count unique words\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling Method Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best results\n",
    "best_f1score = 0\n",
    "best_result = {'resampling_technique': None, 'hyperparameters': None, 'f1-score': None}\n",
    "\n",
    "# Data resampling techniques\n",
    "resampling_techniques = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42),\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {'resampling_technique': [], 'hyperparameters': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1-score': []}\n",
    "\n",
    "# Hyperparameters for BiLSTM model\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.1],\n",
    "    'batch_size': [16],\n",
    "    'optimizer': ['adam'],\n",
    "    'bilstm_neuron': [32],\n",
    "    'neuron': [32],\n",
    "    'lr': [0.01]\n",
    "}\n",
    "\n",
    "# Loop through resampling techniques\n",
    "for resampling_name, resampling_technique in resampling_techniques.items():\n",
    "\n",
    "    # Apply resampling to the entire dataset\n",
    "    X_resampled, y_resampled = resampling_technique.fit_resample(X_padded, y_encoded)\n",
    "    # Sample Counter\n",
    "    print('\\nSample Counter with', resampling_name, \":\")\n",
    "    counter1 = Counter(y_encoded)\n",
    "    print('Before Resampling\\n', counter1)\n",
    "    counter2 = Counter(y_resampled)\n",
    "    print('After Resampling\\n', counter2)\n",
    "\n",
    "    # Plot the class distribution before and after resampling\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(counter1.keys(), counter1.values(), color='blue')\n",
    "    plt.title('Class Distribution Before Resampling')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(counter2.keys(), counter2.values(), color='green')\n",
    "    plt.title('Class Distribution After Resampling')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Split the data into 80% training and 20% temporary data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "    # Further split the temporary data into 50% validation and 50% testing\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    # Loop through hyperparameters\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"\\nResampling Technique: {resampling_name}, Hyperparameters: {params}\")\n",
    "\n",
    "        # Build BiLSTM model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "        model.add(Bidirectional(LSTM(params['bilstm_neuron'])))\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        model.add(Dense(params['neuron'], activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the specified optimizer and learning rate\n",
    "        optimizer = Adam(learning_rate=params['lr']) if params['optimizer'] == 'adam' else params['optimizer']\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True, verbose=1)\n",
    "\n",
    "        # Train model\n",
    "        print('\\nTraining in Process . . .')\n",
    "        history = model.fit(X_train, y_train, epochs=100, batch_size=params['batch_size'], validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=1)\n",
    "        print('Training Done')\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        _, accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f'Accuracy on test data: {accuracy}')\n",
    "\n",
    "        # Plotting loss and accuracy graphs\n",
    "        def plot_graphs(history):\n",
    "            plt.figure(figsize=(16, 5))\n",
    "            # Plot Accuracy\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['accuracy'])\n",
    "            plt.plot(history.history['val_accuracy'])\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.legend(['accuracy', 'val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "\n",
    "            # Plot Loss\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(['loss', 'val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        print('\\nAccuracy and Loss Plot')\n",
    "        plot_graphs(history)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        precision = report['1']['precision']\n",
    "        recall = report['1']['recall']\n",
    "        f1 = report['1']['f1-score']\n",
    "\n",
    "        # Create a confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        # Plot the confusion matrix\n",
    "        def plot_confusion_matrix(conf_matrix):\n",
    "            plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.colorbar()\n",
    "            classes = ['Not Sarcasm', 'Sarcasm']\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes, rotation=45)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.show()\n",
    "\n",
    "        print('\\nConfusion Matrix:')\n",
    "        plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "        # Update results_dict\n",
    "        results_dict['resampling_technique'].append(resampling_name)\n",
    "        results_dict['hyperparameters'].append(params)\n",
    "        results_dict['accuracy'].append(accuracy)\n",
    "        results_dict['precision'].append(precision)\n",
    "        results_dict['recall'].append(recall)\n",
    "        results_dict['f1-score'].append(f1)\n",
    "\n",
    "        # Check if current result is the best for F1-score\n",
    "        if f1 > best_f1score:\n",
    "            best_f1score = f1\n",
    "            best_result = {'resampling_technique': resampling_name, 'hyperparameters': params, 'f1-score': best_f1score}\n",
    "\n",
    "# Print the best result\n",
    "print(\"\\nBest Result:\")\n",
    "print(f\"Resampling Technique: {best_result['resampling_technique']}\")\n",
    "print(f\"Hyperparameters: {best_result['hyperparameters']}\")\n",
    "print(f\"F1-score: {best_result['f1-score']}\")\n",
    "\n",
    "# Print compiled results table\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "print(\"\\nCompiled Results Table:\")\n",
    "print(results_df)\n",
    "results_df.to_csv('results1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best results\n",
    "best_f1score = 0\n",
    "best_result = {'resampling_technique': None, 'hyperparameters': None, 'f1-score': None}\n",
    "\n",
    "# Data resampling techniques\n",
    "resampling_techniques = {\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {'resampling_technique': [], 'hyperparameters': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1-score': []}\n",
    "\n",
    "# Hyperparameters for BiLSTM model\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'optimizer': ['adam'],\n",
    "    'bilstm_neuron': [32, 64, 128],\n",
    "    'neuron': [32, 64, 128],\n",
    "    'lr': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Loop through resampling techniques\n",
    "for resampling_name, resampling_technique in resampling_techniques.items():\n",
    "\n",
    "    # Apply resampling to the entire dataset\n",
    "    X_resampled, y_resampled = resampling_technique.fit_resample(X_padded, y_encoded)\n",
    "    # Sample Counter\n",
    "    print('\\nSample Counter with', resampling_name, \":\")\n",
    "    counter1 = Counter(y_encoded)\n",
    "    print('Before Resampling\\n', counter1)\n",
    "    counter2 = Counter(y_resampled)\n",
    "    print('After Resampling\\n', counter2)\n",
    "\n",
    "    # Plot the class distribution before and after resampling\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(counter1.keys(), counter1.values(), color='blue')\n",
    "    plt.title('Class Distribution Before Resampling')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(counter2.keys(), counter2.values(), color='green')\n",
    "    plt.title('Class Distribution After Resampling')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Data splitting 80:10:10\n",
    "    # Split the data into 80% training and 20% temporary data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "    # Further split the temporary data into 50% validation and 50% testing\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "    # Loop through hyperparameters\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"\\nResampling Technique: {resampling_name}, Hyperparameters: {params}\")\n",
    "\n",
    "        # Build BiLSTM model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "        model.add(Bidirectional(LSTM(params['bilstm_neuron'])))\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        model.add(Dense(params['neuron'], activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the specified optimizer and learning rate\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = Adam(learning_rate=params['lr'])\n",
    "        else:\n",
    "            optimizer = params['optimizer']\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True, verbose=1)\n",
    "\n",
    "        # Train model\n",
    "        print('\\nTraining in Process . . .')\n",
    "        history = model.fit(X_train, y_train, epochs=100, batch_size=params['batch_size'], validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=1)\n",
    "        print('Training Done')\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f'Loss on test data: {loss}')\n",
    "        print(f'Accuracy on test data: {accuracy}')\n",
    "\n",
    "        # Plotting loss and accuracy graphs\n",
    "        # Plot Loss and Accuracy\n",
    "        def plot_graphs(history):\n",
    "            plt.figure(figsize=(16, 5))\n",
    "            # Plot Accuracy\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['accuracy'])\n",
    "            plt.plot(history.history['val_accuracy'])\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.legend(['accuracy', 'val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "\n",
    "            # Plot Loss\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend(['loss', 'val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "\n",
    "            # Place the title in the center above the subplots\n",
    "            plt.show()\n",
    "\n",
    "        print('\\nAccuracy and Loss Plot')\n",
    "        plot_graphs(history)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "        # Thresholding to get predicted classes (0 or 1)\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        precision = report['1']['precision']\n",
    "        recall = report['1']['recall']\n",
    "        f1 = report['1']['f1-score']\n",
    "\n",
    "        # Create a confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        # Plot the confusion matrix\n",
    "        def plot_confusion_matrix(conf_matrix):\n",
    "            plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.colorbar()\n",
    "            classes = ['Not Sarcasm', 'Sarcasm']\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes, rotation=45)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "\n",
    "            # Add text to each cell in the matrix\n",
    "            for i in range(len(classes)):\n",
    "                for j in range(len(classes)):\n",
    "                    text_color = 'black' if i == 1 and j == 0 or i == 0 and j == 1 else 'white'\n",
    "                    plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center', color=text_color)\n",
    "            plt.show()\n",
    "\n",
    "        print('\\nConfusion Matrix:')\n",
    "        print(conf_matrix)\n",
    "        plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "        # Update results_dict\n",
    "        results_dict['resampling_technique'].append(resampling_name)\n",
    "        results_dict['hyperparameters'].append(params)\n",
    "        results_dict['accuracy'].append(accuracy)\n",
    "        results_dict['precision'].append(precision)\n",
    "        results_dict['recall'].append(recall)\n",
    "        results_dict['f1-score'].append(f1)\n",
    "\n",
    "        # Check if current result is the best for both ROC-AUC and F1-score\n",
    "        if f1 > best_f1score:\n",
    "            best_f1score = f1\n",
    "            best_result = {'resampling_technique': resampling_name, 'hyperparameters': params, 'f1-score': best_f1score}\n",
    "\n",
    "# Print the best result\n",
    "print(\"\\nBest Result:\")\n",
    "print(f\"Resampling Technique: {best_result['resampling_technique']}\")\n",
    "print(f\"Hyperparameters: {best_result['hyperparameters']}\")\n",
    "print(f\"F1-score: {best_result['f1-score']}\")\n",
    "\n",
    "# Print compiled results table\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "print(\"\\nCompiled Results Table:\")\n",
    "print(results_df)\n",
    "results_df.to_csv('results2.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
